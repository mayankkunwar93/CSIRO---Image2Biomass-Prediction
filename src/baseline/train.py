# -*- coding: utf-8 -*-
"""train

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ts-hCAj2lweJ7DEw8EAeBsNbikPk-4Bc
"""

import os, warnings, torch
import numpy as np
import pandas as pd
from torch.utils.data import DataLoader
from sklearn.model_selection import KFold
from torch.optim.lr_scheduler import CosineAnnealingLR

# Local Imports
from models import DinoHierarchicalRegressor
from dataset import GrassRGBDataset
from utils import weighted_mse_loss, weighted_r2_score

warnings.simplefilter("ignore")
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Paths
BASE_DIR = "/content/drive/MyDrive/mayank/CSIRO - Image2Biomass Prediction"
CSV_PATH = f"{BASE_DIR}/train.csv"
DINO_WEIGHTS = "/content/drive/MyDrive/mayank/dinov2_vits14_pretrain.pth"
LOCAL_HUB = "/content/drive/MyDrive/dinov2"

# Hyperparameters
IMG_SIZE = 980
BATCH_SIZE = 32
EPOCHS = 50

def main():
    df_raw = pd.read_csv(CSV_PATH)
    targets_to_use = ["Dry_Clover_g", "Dry_Dead_g", "Dry_Green_g", "Dry_Total_g", "GDM_g"]

    # Prep Data
    df = df_raw[df_raw["target_name"].isin(targets_to_use)]
    df = df.pivot(index="image_path", columns="target_name", values="target").reset_index()

    targets = np.log1p(df[targets_to_use].values.astype(np.float32))
    t_mean, t_std = targets.mean(axis=0), targets.std(axis=0) + 1e-8
    df["target"] = list((targets - t_mean) / t_std)

    kf = KFold(n_splits=5, shuffle=True, random_state=42)

    for fold, (tr_idx, va_idx) in enumerate(kf.split(df)):
        print(f"\n===== FOLD {fold+1} =====")

        train_loader = DataLoader(GrassRGBDataset(df.iloc[tr_idx], BASE_DIR, IMG_SIZE, train=True),
                                  batch_size=BATCH_SIZE, shuffle=True)
        val_loader = DataLoader(GrassRGBDataset(df.iloc[va_idx], BASE_DIR, IMG_SIZE, train=False),
                                batch_size=BATCH_SIZE, shuffle=False)

        model = DinoHierarchicalRegressor(DINO_WEIGHTS, LOCAL_HUB).to(DEVICE)
        opt = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=3e-5)
        scheduler = CosineAnnealingLR(opt, T_max=EPOCHS-1)

        best_wr2 = -1
        for epoch in range(EPOCHS):
            model.train()
            for x, y in train_loader:
                x, y = x.to(DEVICE), y.to(DEVICE)
                opt.zero_grad()
                loss = weighted_mse_loss(model(x), y, DEVICE)
                loss.backward()
                opt.step()

            scheduler.step()
            # Add validation logic here...
            print(f"Epoch {epoch+1} complete.")

if __name__ == "__main__":
    main()